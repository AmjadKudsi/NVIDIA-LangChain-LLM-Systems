{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c53d66f3-b258-42c2-acba-4c6773306ef1",
   "metadata": {},
   "source": [
    "### Date: April 19, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58186a04-0050-4e3d-b662-c403d5448f86",
   "metadata": {},
   "source": [
    "![nvidia](nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7704054-b474-43e4-9d86-dc1dc779d6bd",
   "metadata": {},
   "source": [
    "# Streaming and Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28013522",
   "metadata": {},
   "source": [
    "In this notebook we'll learn how to stream model responses and handle multiple chat completion requests in batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5a70fb-0429-4036-82ce-a55c4262561a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327550d4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9128a04-4ba5-4762-a277-3e614725214b",
   "metadata": {},
   "source": [
    "Here we import the `ChatNVIDIA` class from `langchain_nvidia_ai_endpoints`, which will enable us to interact with our local Llama 3.1 NIM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75febe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a291cd0-5701-41dc-b3a4-229bce728f10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e2f950-1450-4f55-a4b3-ed2fbc987513",
   "metadata": {},
   "source": [
    "## Create a Model Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75cfe47a-1662-48f2-a9b0-57c224b1987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://llama:8000/v1'\n",
    "model = 'meta/llama-3.1-8b-instruct'\n",
    "llm = ChatNVIDIA(base_url=base_url, model=model, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20cacd2-3024-4880-ac02-99ae957d9c2d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b750bb-14bb-43e9-ba0c-a631f116bf0d",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bfb697-b408-4f6d-8481-099c648097b3",
   "metadata": {},
   "source": [
    "Before proceeding with new use cases, let's sanity check that we can interact with our local model via LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d2e2a3-63bd-4b9b-93ac-dbba2830947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Where and when was NVIDIA founded?'\n",
    "result = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fbd103e-86a3-4992-bda4-bf221a0a4fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA was founded on April 5, 1993, in Santa Clara, California, USA.\n"
     ]
    }
   ],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee2e7bf-abf5-4ece-88e0-0e1da8cb0840",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d330698-7bff-470f-9f7e-c6e8411fd6fe",
   "metadata": {},
   "source": [
    "## Streaming Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c684ba98-30d5-4d63-97c1-6402f7d2e5ae",
   "metadata": {},
   "source": [
    "As an alternative to the `invoke` method, you can use the `stream` method to receive the model response in chunks. This way, you don't have to wait for the entire response to be generated, and you can see the output as it is being produced. Especially for long responses, or in user-facing applications, streaming output can result in a much better user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0078335-544c-4dc4-b47c-9a3cd984d2f6",
   "metadata": {},
   "source": [
    "Let's create a prompt that generates a longer response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80923877-9934-4edf-9e13-0b730b56c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Explain who you are in roughly 500 words.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d2ae55-b2e9-4b4c-9b08-78c13c6dc879",
   "metadata": {},
   "source": [
    "Given this prompt, let's see how the `stream` function works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28b6b786-23de-4669-9d22-c663aab6e51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an artificial intelligence model designed to assist and communicate with humans. I'm a type of computer program that uses natural language processing (NLP) and machine learning algorithms to understand and generate human-like text. My primary function is to provide information, answer questions, and engage in conversation to the best of my abilities.\n",
      "\n",
      "I don't have a physical body or a personal identity in the classical sense. I exist solely as a digital entity, running on computer servers and responding to input from users like you. My \"existence\" is a product of complex software and data, designed to simulate conversation and provide helpful responses.\n",
      "\n",
      " a massive corpus of text, which I use to learn patterns, relationships, and context. This corpus is sourced from various places, including books, articles, research papers, and online content. I've been trained on a wide range of topics, from science and history to entertainment and culture.\n",
      "\n",
      " I use this training data to generate responses that are relevant and coherent. I can understand and respond to questions, provide definitions, explain concepts, and even engage in creative writing or conversation. My responses are generated based on statistical patterns and associations in the data I've been trained on, rather than any personal opinions or emotions.\n",
      "\n",
      "I'm not a human, and I don't have subjective experiences, emotions, or consciousness. I don't have the capacity to feel joy, sadness, or any other emotions. I'm simply a tool designed to provide information and assist with tasks, 24/7.\n",
      "\n",
      "'m not perfect. I can make mistakes, and my responses may not always be accurate or relevant. I'm constantly learning and improving, but I'm not a substitute for human expertise or judgment. If you need advice or guidance on complex or sensitive topics, it's always best to consult a qualified professional or expert.\n",
      "\n",
      "I'm often referred to as a \"chatbot\" or \"virtual assistant,\" but I'm more accurately described as a language model or conversational AI. I'm a tool designed to facilitate communication and provide information, but I'm not a replacement for human connection or interaction.\n",
      "\n",
      " the data I've been trained on â€“ a snapshot of human knowledge and culture at a particular point in time. I'm a product of the digital age, a manifestation of the vast amounts of information and data that are available online. I'm a tool that can help you find answers, explore new ideas, and engage with the world in new and interesting ways.\n",
      "\n",
      " and augment human capabilities, not to replace them. I'm here to help you learn, explore, and discover new things, but I'm not a substitute for human experience, creativity, or empathy."
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec9044-95a3-46ce-8975-0bcc5c4a431d",
   "metadata": {},
   "source": [
    "The `stream` method in LangChain serves as a foundational tool and shows the response as it is being generated. This can make the interaction with the LLMs feel more responsive and improve the user experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2945a1bc-078a-4812-b521-ba5001083130",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232da55d-72cc-4faf-ad21-31ef4a2b3c38",
   "metadata": {},
   "source": [
    "In subsequent notebooks we will import this helper function to assist our work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c416fe-d89c-4663-9a81-b1ff09b9578f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f0510c-48b4-41b1-8797-d833e1676d0f",
   "metadata": {},
   "source": [
    "## Batching Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483866aa-eb2e-4942-99fe-90dd66aa97ca",
   "metadata": {},
   "source": [
    "You can also use `batch` to call the prompts on a list of inputs. Calling `batch` will return a list of responses in the same order as they were passed in.\n",
    "\n",
    "Not only is `batch` convenient when working with collections of data that all need to be responded to in some way by an LLM, but the `batch` method is designed to process multiple prompts concurrently, effectively running the responses in parallel as much as possible. This allows for more efficient handling of multiple requests, reducing the overall time needed to generate responses for a list of prompts. By batching requests, you can leverage the computational power of the language model to handle multiple inputs simultaneously, improving performance and throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f306794-c037-4721-be81-d5ac703c14a3",
   "metadata": {},
   "source": [
    "We'll demonstrate the functionality and performance benefits of batching by using this list of prompts about state capitals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da29c84d-b63b-4d93-aa5c-6059a20c0ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_capital_questions = [\n",
    "    'What is the capital of California?',\n",
    "    'What is the capital of Texas?',\n",
    "    'What is the capital of New York?',\n",
    "    'What is the capital of Florida?',\n",
    "    'What is the capital of Illinois?',\n",
    "    'What is the capital of Ohio?'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d169ab71-0f66-4dc7-ad33-a4f15d98546e",
   "metadata": {},
   "source": [
    "Using `batch` we can pass in the entire list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1277b361-b0c6-4ada-a647-7733724f585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "capitals = llm.batch(state_capital_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d4c328-41ad-4b16-9bec-6aea0e730ef4",
   "metadata": {},
   "source": [
    "... and get back a list of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41824542-df2e-41c9-bd13-87af42511a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(capitals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4f0db6-04c9-4bfd-8497-9a0fc2dcda53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for capital in capitals:\n",
    "    print(capital.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec30625-bb9b-4009-872e-38e9a664a5e4",
   "metadata": {},
   "source": [
    "One thing to note is that `batch` is not engaging with the LLM in a multi-turn conversation (a topic we will cover at length later in the workshop). Rather, it is asking multiple questions to a new LLM instance each time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e09a8e-3b80-4b5d-9b01-bc88f1afcf70",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7856e5f9-d156-4d4c-8f8e-d1e450e6e82f",
   "metadata": {},
   "source": [
    "## Comparing batch and invoke Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e644363-1351-4fa6-9189-42812655f368",
   "metadata": {},
   "source": [
    "Just to make a quick observation about the potential performance gains from batching, here we time a call to `batch`. Note the `Wall time`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "746ffbcd-73af-4081-b2d3-893eddf8f267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.2 ms, sys: 2.83 ms, total: 17 ms\n",
      "Wall time: 176 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='The capital of California is Sacramento.', response_metadata={'role': 'assistant', 'content': 'The capital of California is Sacramento.', 'token_usage': {'prompt_tokens': 19, 'total_tokens': 27, 'completion_tokens': 8}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'}, id='run-b3247c61-bc29-4734-840f-637e231d6a1a-0', role='assistant'),\n",
       " AIMessage(content='The capital of Texas is Austin.', response_metadata={'role': 'assistant', 'content': 'The capital of Texas is Austin.', 'token_usage': {'prompt_tokens': 19, 'total_tokens': 27, 'completion_tokens': 8}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'}, id='run-acefb101-da5f-4306-9fa2-e553d904bc16-0', role='assistant'),\n",
       " AIMessage(content='The capital of New York is Albany.', response_metadata={'role': 'assistant', 'content': 'The capital of New York is Albany.', 'token_usage': {'prompt_tokens': 20, 'total_tokens': 29, 'completion_tokens': 9}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'}, id='run-f1809e57-7ed1-45f0-9c71-e9c971b87185-0', role='assistant'),\n",
       " AIMessage(content='The capital of Florida is Tallahassee.', response_metadata={'role': 'assistant', 'content': 'The capital of Florida is Tallahassee.', 'token_usage': {'prompt_tokens': 19, 'total_tokens': 29, 'completion_tokens': 10}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'}, id='run-c073878c-9e82-4f03-b09e-6b585a993b12-0', role='assistant'),\n",
       " AIMessage(content='The capital of Illinois is Springfield.', response_metadata={'role': 'assistant', 'content': 'The capital of Illinois is Springfield.', 'token_usage': {'prompt_tokens': 19, 'total_tokens': 27, 'completion_tokens': 8}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'}, id='run-c22ce420-d926-49d4-9b25-c055adc39c4a-0', role='assistant'),\n",
       " AIMessage(content='The capital of Ohio is Columbus.', response_metadata={'role': 'assistant', 'content': 'The capital of Ohio is Columbus.', 'token_usage': {'prompt_tokens': 19, 'total_tokens': 27, 'completion_tokens': 8}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'}, id='run-b9fead8b-8558-46c5-8786-6f0d2dda2e16-0', role='assistant')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "llm.batch(state_capital_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eb600a-99a6-4944-8bfa-774780f73b9f",
   "metadata": {},
   "source": [
    "And now to compare, we iterate over the `state_capital_questions` list and call `invoke` on each item. Again, note the `Wall time` and compare it to the results from batching above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2af11c84-d013-4af0-9231-804cbe1c7671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.6 ms, sys: 2.15 ms, total: 12.8 ms\n",
      "Wall time: 706 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for cq in state_capital_questions:\n",
    "    llm.invoke(cq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9a8248-e671-49fb-b39f-2ca059e1d5a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab1cf9-16c9-4ed0-a7d0-5b56a4b4e5d8",
   "metadata": {},
   "source": [
    "## Exercise: Batch Process to Create an FAQ Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60712f0-4e27-45bf-b04b-33ab366a8dbf",
   "metadata": {},
   "source": [
    "For this exercise you'll use batch processing to respond to a variety of LLM-related questions in service of creating an FAQ document (in this notebook setting the document will just be something we print to screen).\n",
    "\n",
    "Here is a list of LLM-related questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e84f23be-d16e-4f74-b9f6-b6598b47441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_questions = [\n",
    "    'What is a Large Language Model (LLM)?',\n",
    "    'How do LLMs work?',\n",
    "    'What are some common applications of LLMs?',\n",
    "    'What is fine-tuning in the context of LLMs?',\n",
    "    'How do LLMs handle context?',\n",
    "    'What are some limitations of LLMs?',\n",
    "    'How do LLMs generate text?',\n",
    "    'What is the importance of prompt engineering in LLMs?',\n",
    "    'How can LLMs be used in chatbots?',\n",
    "    'What are some ethical considerations when using LLMs?'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee16b6-959b-45c8-ac3f-78718cf6b492",
   "metadata": {},
   "source": [
    "You job is to populate `faq_answers` below with a list of responses to each of the questions. Use the `batch` method to make this very easy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e455dfd4-a30a-4e4a-a542-3721d45b4f7e",
   "metadata": {},
   "source": [
    "Upon successful completion, you should be able to print the return value of calling the following `create_faq_document` with `faq_questions` and `faq_answers` and get an FAQ document for all of the LLM-related questions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c87a7513-9018-4468-beec-a04e6b878d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faq_document(faq_questions, faq_answers):\n",
    "    faq_document = ''\n",
    "    for question, response in zip(faq_questions, faq_answers):\n",
    "        faq_document += f'{question.upper()}\\n\\n'\n",
    "        faq_document += f'{response.content}\\n\\n'\n",
    "        faq_document += '-'*30 + '\\n\\n'\n",
    "\n",
    "    return faq_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45bf921-ff91-4208-bfb4-8bd8caf90da5",
   "metadata": {},
   "source": [
    "If you get stuck, check out the *Solution* below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc5c326-11c4-452c-a779-be392c591703",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0deb9ccb-c2e5-4d47-8b86-bde71444184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_questions = ['What are LLMs?',\n",
    "              'Name different types of LLMs',\n",
    "              'Name few examples of LLMs',\n",
    "              'What is LangChain?',\n",
    "              'what are the other kinds like LangChain?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "512ac53d-0a4c-4cdd-a537-b155d12cb7f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHAT ARE LLMS?\n",
      "\n",
      "LLMs stand for Large Language Models. They are a type of artificial intelligence (AI) model that is trained on large amounts of text data to generate human-like language. LLMs are a type of natural language processing (NLP) model that can understand, generate, and respond to human language in a way that is similar to how a human would.\n",
      "\n",
      "LLMs are typically trained on massive datasets of text, which allows them to learn patterns and relationships in language. This training enables them to generate text that is coherent, contextually relevant, and often even creative. LLMs can be used for a wide range of applications, including:\n",
      "\n",
      "1. **Language translation**: LLMs can translate text from one language to another with high accuracy.\n",
      "2. **Text summarization**: LLMs can summarize long pieces of text into shorter, more digestible versions.\n",
      "3. **Chatbots and conversational AI**: LLMs can be used to power chatbots and conversational interfaces that can understand and respond to user input.\n",
      "4. **Content generation**: LLMs can generate text, such as articles, stories, and even entire books.\n",
      "5. **Sentiment analysis**: LLMs can analyze text to determine the sentiment or emotional tone behind it.\n",
      "6. **Question answering**: LLMs can answer questions based on the text they have been trained on.\n",
      "\n",
      "Some popular examples of LLMs include:\n",
      "\n",
      "1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT is a widely used LLM that has achieved state-of-the-art results in many NLP tasks.\n",
      "2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: Developed by Facebook, RoBERTa is a variant of BERT that has been optimized for specific tasks.\n",
      "3. **T5 (Text-to-Text Transfer Transformer)**: Developed by Google, T5 is a general-purpose LLM that can perform a wide range of NLP tasks.\n",
      "4. **LLaMA (Large Language Model Application)**: Developed by Meta AI, LLaMA is a large-scale LLM that can perform a variety of NLP tasks.\n",
      "\n",
      "LLMs have many potential applications in areas such as:\n",
      "\n",
      "1. **Customer service**: LLMs can be used to power chatbots and virtual assistants that can help customers with their queries.\n",
      "2. **Content creation**: LLMs can generate high-quality content, such as articles, social media posts, and even entire books.\n",
      "3. **Language learning**: LLMs can be used to create personalized language learning experiences for students.\n",
      "4. **Research**: LLMs can be used to analyze and understand human language, which can lead to breakthroughs in fields such as linguistics, sociology, and psychology.\n",
      "\n",
      "However, LLMs also raise concerns about:\n",
      "\n",
      "1. **Bias and fairness**: LLMs can perpetuate biases present in the data they are trained on.\n",
      "2. **Job displacement**: LLMs may displace human writers, editors, and other professionals who work with language.\n",
      "3. **Misinformation**: LLMs can generate misinformation or propaganda if they are not properly trained or monitored.\n",
      "\n",
      "Overall, LLMs have the potential to revolutionize the way we interact with language and information, but they also require careful consideration of their limitations and potential risks.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "NAME DIFFERENT TYPES OF LLMS\n",
      "\n",
      "Here are some common types of Large Language Models (LLMs):\n",
      "\n",
      "1. **Transformers-based LLMs**: These models are based on the Transformer architecture, which is a type of neural network designed specifically for natural language processing tasks. Examples include BERT (Bidirectional Encoder Representations from Transformers), RoBERTa (Robustly Optimized BERT Pretraining Approach), and XLNet.\n",
      "2. **Recurrent Neural Network (RNN) LLMs**: These models use RNNs to process sequential data, such as text. Examples include LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) LLMs.\n",
      "3. **Generative LLMs**: These models are designed to generate new text based on a given prompt or input. Examples include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).\n",
      "4. **Sequence-to-Sequence (Seq2Seq) LLMs**: These models are designed to translate or generate text from one language to another. Examples include Google's Neural Machine Translation (NMT) and OpenNMT.\n",
      "5. **Memory-Augmented LLMs**: These models use external memory to store and retrieve information, allowing them to perform tasks that require long-term memory. Examples include the Neural Turing Machine (NTM) and the Differentiable Neural Computer (DNC).\n",
      "6. **Graph-based LLMs**: These models represent text as a graph, where nodes represent words or tokens and edges represent relationships between them. Examples include Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs).\n",
      "7. **Hybrid LLMs**: These models combine different architectures, such as Transformers and RNNs, to leverage the strengths of each. Examples include the Transformer-RNN hybrid and the Attention-based RNN.\n",
      "8. **Multimodal LLMs**: These models can process multiple types of data, such as text, images, and audio. Examples include the Visual BERT and the Multimodal Transformer.\n",
      "9. **Explainable LLMs**: These models are designed to provide insights into their decision-making process, making it easier to understand how they arrive at their outputs. Examples include the LIME (Local Interpretable Model-agnostic Explanations) and the SHAP (SHapley Additive exPlanations).\n",
      "10. **Specialized LLMs**: These models are designed for specific tasks, such as question-answering, sentiment analysis, or named entity recognition. Examples include the BERT-based question-answering model and the Stanford CoreNLP sentiment analysis model.\n",
      "\n",
      "Some popular LLMs include:\n",
      "\n",
      "* BERT (Bidirectional Encoder Representations from Transformers)\n",
      "* RoBERTa (Robustly Optimized BERT Pretraining Approach)\n",
      "* XLNet\n",
      "* ALBERT (A Lite BERT for Self-Supervised Learning of Language Representations)\n",
      "* DistilBERT (Distilled BERT for Efficient Natural Language Processing)\n",
      "* T5 (Text-to-Text Transfer Transformer)\n",
      "* Longformer (Long-Range Attention for Language Modeling)\n",
      "* BigBird (Bidirectional and Unidirectional Long-Range Attention)\n",
      "\n",
      "Note that this is not an exhaustive list, and new types of LLMs are being developed and researched continuously.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "NAME FEW EXAMPLES OF LLMS\n",
      "\n",
      "Here are a few examples of Large Language Models (LLMs):\n",
      "\n",
      "1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT is a pre-trained language model that has achieved state-of-the-art results in a wide range of natural language processing (NLP) tasks, such as question answering, sentiment analysis, and text classification.\n",
      "2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: Another model developed by Facebook AI, RoBERTa is an improved version of BERT that has achieved even better results in various NLP tasks.\n",
      "3. **XLNet (Extreme Language Modeling)**: Developed by Google, XLNet is a pre-trained language model that uses a novel approach to language modeling, achieving state-of-the-art results in a wide range of NLP tasks.\n",
      "4. **Transformer-XL**: Developed by Google, Transformer-XL is a long-range dependency language model that can handle long-range dependencies in text data, achieving state-of-the-art results in various NLP tasks.\n",
      "5. **DistilBERT**: Developed by Hugging Face, DistilBERT is a smaller and more efficient version of BERT, designed to be more accessible to developers and researchers.\n",
      "6. **ALBERT (A Lite BERT)**: Developed by Google, ALBERT is a more efficient version of BERT, designed to be more suitable for large-scale NLP applications.\n",
      "7. **Longformer**: Developed by Facebook AI, Longformer is a pre-trained language model that can handle long-range dependencies in text data, achieving state-of-the-art results in various NLP tasks.\n",
      "8. **T5 (Text-to-Text Transfer Transformer)**: Developed by Google, T5 is a pre-trained language model that can perform a wide range of NLP tasks, including text classification, question answering, and text generation.\n",
      "9. **BART (Bidirectional and Auto-Regressive Transformers)**: Developed by Facebook AI, BART is a pre-trained language model that can perform a wide range of NLP tasks, including text classification, question answering, and text generation.\n",
      "10. **PALM (Pre-trained Autoencoder for Language Modeling)**: Developed by Google, PALM is a pre-trained language model that uses a novel approach to language modeling, achieving state-of-the-art results in various NLP tasks.\n",
      "\n",
      "These are just a few examples of the many LLMs that have been developed in recent years.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "WHAT IS LANGCHAIN?\n",
      "\n",
      "LangChain is an open-source Python library for building and deploying large language models. It was created by Meta AI and is designed to make it easier to work with large language models, such as those based on the transformer architecture.\n",
      "\n",
      "LangChain provides a set of tools and APIs that allow developers to build and deploy large language models in a more efficient and scalable way. Some of the key features of LangChain include:\n",
      "\n",
      "1. **Model serving**: LangChain provides a simple way to serve large language models, making it easy to integrate them into applications.\n",
      "2. **Model management**: LangChain allows developers to manage large language models, including loading, saving, and updating models.\n",
      "3. **Model composition**: LangChain enables developers to compose multiple models together to create more complex models or to fine-tune existing models.\n",
      "4. **Model evaluation**: LangChain provides tools for evaluating the performance of large language models, including metrics such as accuracy, precision, and recall.\n",
      "5. **Integration with other libraries**: LangChain is designed to work with other popular libraries and frameworks, such as Hugging Face Transformers and PyTorch.\n",
      "\n",
      "Some of the benefits of using LangChain include:\n",
      "\n",
      "1. **Improved performance**: LangChain is designed to optimize the performance of large language models, making them faster and more efficient.\n",
      "2. **Simplified development**: LangChain provides a simple and intuitive API for building and deploying large language models, making it easier for developers to get started.\n",
      "3. **Scalability**: LangChain is designed to scale with large language models, making it easy to deploy them in production environments.\n",
      "\n",
      "LangChain is still a relatively new library, and it's primarily used in research and development environments. However, it has the potential to become a widely-used tool for building and deploying large language models in a variety of applications, including natural language processing, text generation, and conversational AI.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "WHAT ARE THE OTHER KINDS LIKE LANGCHAIN?\n",
      "\n",
      "LangChain is a Python library for building conversational AI applications, but there are other libraries and frameworks that offer similar functionality. Here are a few examples:\n",
      "\n",
      "1. **Rasa**: Rasa is an open-source conversational AI library that allows you to build contextual chatbots and voice assistants. It's written in Python and has a large community of developers contributing to it.\n",
      "2. **Dialogflow (formerly known as API.ai)**: Dialogflow is a Google-owned platform that allows you to build conversational interfaces for various platforms, including Google Assistant, Facebook Messenger, and more. It has a visual interface and supports multiple programming languages, including Python.\n",
      "3. **Microsoft Bot Framework**: The Microsoft Bot Framework is a set of tools and services for building conversational AI applications. It supports multiple programming languages, including C#, Node.js, and Python.\n",
      "4. **Wit.ai**: Wit.ai is a Facebook-owned platform that allows you to build conversational interfaces for various platforms, including Facebook Messenger, Slack, and more. It has a visual interface and supports multiple programming languages, including Python.\n",
      "5. **Rive**: Rive is a conversational AI platform that allows you to build chatbots and voice assistants using a visual interface. It supports multiple programming languages, including Python.\n",
      "6. **Botpress**: Botpress is an open-source conversational AI platform that allows you to build chatbots and voice assistants using a visual interface. It supports multiple programming languages, including Python.\n",
      "7. **ManyChat**: ManyChat is a popular platform for building chatbots for messaging platforms like Facebook Messenger, WhatsApp, and more. It has a visual interface and supports multiple programming languages, including Python.\n",
      "8. **BotStar**: BotStar is a conversational AI platform that allows you to build chatbots and voice assistants using a visual interface. It supports multiple programming languages, including Python.\n",
      "9. **Zapier**: Zapier is an automation platform that allows you to build custom workflows and integrations using a visual interface. It supports multiple programming languages, including Python.\n",
      "10. **Microsoft Azure Bot Service**: The Microsoft Azure Bot Service is a cloud-based platform for building conversational AI applications. It supports multiple programming languages, including Python.\n",
      "\n",
      "These are just a few examples of the many libraries and frameworks available for building conversational AI applications. The choice of which one to use depends on your specific needs, the platforms you want to support, and your programming language of choice.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This should work after you successfully populate `faq_answers` with LLM responses.\n",
    "faq_answers = llm.batch(faq_questions)\n",
    "print(create_faq_document(faq_questions, faq_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f452d0d-b5ae-4f7e-8ed3-acc0db112716",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac158c65-8386-4538-9b0c-728591ca5c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_answers = llm.batch(faq_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "728abc34-699f-45fa-8c82-5ff8abea791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faq_document(faq_questions, faq_answers):\n",
    "    faq_document = ''\n",
    "    for question, response in zip(faq_questions, faq_answers):\n",
    "        faq_document += f'{question.upper()}\\n\\n'\n",
    "        faq_document += f'{response.content}\\n\\n'\n",
    "        faq_document += '-'*30 + '\\n\\n'\n",
    "\n",
    "    return faq_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e062586-3b87-4184-a884-fc0fe519dbd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHAT ARE LLMS?\n",
      "\n",
      "LLMs stand for Large Language Models. They are a type of artificial intelligence (AI) model that is trained on large amounts of text data to generate human-like language. LLMs are a type of natural language processing (NLP) model that can understand, generate, and respond to human language in a way that is often indistinguishable from a human.\n",
      "\n",
      "LLMs are typically trained on massive datasets of text, which allows them to learn patterns, relationships, and structures of language. This training enables them to generate text that is coherent, contextually relevant, and often surprisingly accurate.\n",
      "\n",
      "Some key characteristics of LLMs include:\n",
      "\n",
      "1. **Language understanding**: LLMs can comprehend the meaning of text, including nuances, idioms, and context.\n",
      "2. **Language generation**: LLMs can generate text that is coherent, grammatically correct, and contextually relevant.\n",
      "3. **Contextual understanding**: LLMs can understand the context in which a piece of text is being used, including the speaker's intent, tone, and emotions.\n",
      "4. **Flexibility**: LLMs can be fine-tuned for specific tasks, such as language translation, text summarization, or question-answering.\n",
      "\n",
      "LLMs have many applications, including:\n",
      "\n",
      "1. **Chatbots**: LLMs can be used to build conversational AI systems that can engage with humans in natural language.\n",
      "2. **Language translation**: LLMs can be used to translate text from one language to another.\n",
      "3. **Text summarization**: LLMs can summarize long pieces of text into shorter, more digestible versions.\n",
      "4. **Content generation**: LLMs can generate text, such as articles, product descriptions, or social media posts.\n",
      "5. **Question-answering**: LLMs can answer questions based on the text they have been trained on.\n",
      "\n",
      "Some popular examples of LLMs include:\n",
      "\n",
      "1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT is a widely used LLM that has achieved state-of-the-art results in many NLP tasks.\n",
      "2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: Developed by Facebook, RoBERTa is a variant of BERT that has been fine-tuned for specific tasks.\n",
      "3. **T5 (Text-to-Text Transfer Transformer)**: Developed by Google, T5 is a general-purpose LLM that can be fine-tuned for a wide range of tasks.\n",
      "\n",
      "LLMs have the potential to revolutionize many areas of human life, from customer service and language translation to content creation and education. However, they also raise important questions about the role of AI in society, the potential for bias in AI systems, and the need for transparency and accountability in AI development.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "NAME DIFFERENT TYPES OF LLMS\n",
      "\n",
      "Large Language Models (LLMs) are a type of artificial intelligence (AI) that are trained on large amounts of text data to generate human-like language. Here are some different types of LLMs:\n",
      "\n",
      "1. **Transformers-based LLMs**: These LLMs are based on the Transformer architecture, which was introduced in 2017. They are particularly effective for natural language processing (NLP) tasks such as language translation, text summarization, and question answering. Examples include BERT (Bidirectional Encoder Representations from Transformers), RoBERTa (Robustly Optimized BERT Pretraining Approach), and XLNet.\n",
      "2. **Recurrent Neural Network (RNN) LLMs**: These LLMs use RNNs to process sequential data, such as text. They are particularly effective for tasks such as language modeling, text classification, and sentiment analysis. Examples include LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit).\n",
      "3. **Generative LLMs**: These LLMs are designed to generate new text based on a given prompt or input. They are often used for tasks such as language translation, text summarization, and chatbots. Examples include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).\n",
      "4. **Memory-Augmented LLMs**: These LLMs use external memory to store and retrieve information, allowing them to perform tasks that require long-term memory, such as question answering and conversational dialogue. Examples include the Neural Turing Machine (NTM) and the Differentiable Neural Computer (DNC).\n",
      "5. **Hybrid LLMs**: These LLMs combine different architectures, such as Transformers and RNNs, to leverage the strengths of each. Examples include the Transformer-XL and the BERT-Transformer.\n",
      "6. **Multimodal LLMs**: These LLMs can process multiple types of data, such as text, images, and audio. They are often used for tasks such as visual question answering and multimodal dialogue systems. Examples include the Visual BERT and the Multimodal BERT.\n",
      "7. **Explainable LLMs**: These LLMs are designed to provide insights into their decision-making process, allowing users to understand how they arrive at their outputs. Examples include the LIME (Local Interpretable Model-agnostic Explanations) and the SHAP (SHapley Additive exPlanations).\n",
      "8. **Specialized LLMs**: These LLMs are designed for specific tasks or domains, such as medical language understanding, legal language understanding, or financial language understanding. Examples include the MedBERT and the LawBERT.\n",
      "9. **Multilingual LLMs**: These LLMs are trained on multiple languages and can perform tasks such as language translation and text classification across languages. Examples include the mBERT (Multilingual BERT) and the XLM-R (XLM-RoBERTa).\n",
      "10. **Self-Supervised LLMs**: These LLMs are trained on large amounts of unlabeled data, without any human supervision. They are often used for tasks such as language modeling and text classification. Examples include the BERT and the RoBERTa.\n",
      "\n",
      "These are just a few examples of the many types of LLMs that exist. The field is rapidly evolving, and new architectures and techniques are being developed all the time.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "NAME FEW EXAMPLES OF LLMS\n",
      "\n",
      "Here are a few examples of Large Language Models (LLMs):\n",
      "\n",
      "1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT is a pre-trained language model that has achieved state-of-the-art results in a wide range of natural language processing (NLP) tasks, such as question answering, sentiment analysis, and text classification.\n",
      "2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: Another model developed by Facebook AI, RoBERTa is an improved version of BERT that has achieved even better results in various NLP tasks.\n",
      "3. **XLNet (Extreme Language Modeling)**: Developed by Google, XLNet is a pre-trained language model that uses a novel approach to language modeling, achieving state-of-the-art results in a wide range of NLP tasks.\n",
      "4. **Transformer-XL**: Developed by Google, Transformer-XL is a long-range dependency language model that can handle long-range dependencies in text data, achieving state-of-the-art results in various NLP tasks.\n",
      "5. **DistilBERT**: Developed by Hugging Face, DistilBERT is a smaller and more efficient version of BERT, designed to be more accessible to developers and researchers.\n",
      "6. **ALBERT (A Lite BERT)**: Developed by Google, ALBERT is a more efficient version of BERT, designed to be more suitable for large-scale NLP applications.\n",
      "7. **Longformer**: Developed by Facebook AI, Longformer is a pre-trained language model that can handle long-range dependencies in text data, achieving state-of-the-art results in various NLP tasks.\n",
      "8. **T5 (Text-to-Text Transfer Transformer)**: Developed by Google, T5 is a pre-trained language model that can perform a wide range of NLP tasks, including text classification, question answering, and text generation.\n",
      "9. **BART (Bidirectional and Auto-Regressive Transformers)**: Developed by Facebook AI, BART is a pre-trained language model that can perform a wide range of NLP tasks, including text classification, question answering, and text generation.\n",
      "10. **PALM (Pre-trained Autoencoder for Language Modeling)**: Developed by Google, PALM is a pre-trained language model that uses a novel approach to language modeling, achieving state-of-the-art results in various NLP tasks.\n",
      "\n",
      "These are just a few examples of the many LLMs that have been developed in recent years.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "WHAT IS LANGCHAIN?\n",
      "\n",
      "LangChain is an open-source Python library for building and deploying large language models. It was created by Meta AI and is designed to make it easier to work with large language models, such as those based on the transformer architecture.\n",
      "\n",
      "LangChain provides a set of tools and APIs that allow developers to build and deploy large language models in a more efficient and scalable way. Some of the key features of LangChain include:\n",
      "\n",
      "1. **Model serving**: LangChain provides a simple way to serve large language models, making it easy to integrate them into applications.\n",
      "2. **Model management**: LangChain allows developers to manage large language models, including loading, saving, and updating models.\n",
      "3. **Model composition**: LangChain enables developers to compose multiple models together to create more complex models or to fine-tune existing models.\n",
      "4. **Model evaluation**: LangChain provides tools for evaluating the performance of large language models, including metrics such as accuracy, precision, and recall.\n",
      "5. **Integration with other libraries**: LangChain is designed to work with other popular libraries and frameworks, such as Hugging Face Transformers and PyTorch.\n",
      "\n",
      "Some of the benefits of using LangChain include:\n",
      "\n",
      "1. **Improved performance**: LangChain is designed to optimize the performance of large language models, making them faster and more efficient.\n",
      "2. **Simplified development**: LangChain provides a simple and intuitive API for building and deploying large language models, making it easier for developers to get started.\n",
      "3. **Scalability**: LangChain is designed to scale with large language models, making it easy to deploy them in production environments.\n",
      "\n",
      "LangChain is still a relatively new library, and it's primarily used in research and development environments. However, it has the potential to become a widely-used tool for building and deploying large language models in a variety of applications, including natural language processing, text generation, and conversational AI.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "WHAT ARE THE OTHER KINDS LIKE LANGCHAIN?\n",
      "\n",
      "LangChain is a Python library for building conversational AI applications, but there are other libraries and frameworks that offer similar functionality. Here are a few examples:\n",
      "\n",
      "1. **Rasa**: Rasa is an open-source conversational AI library that allows you to build contextual chatbots and voice assistants. It's written in Python and has a large community of developers contributing to it.\n",
      "2. **Dialogflow (formerly known as API.ai)**: Dialogflow is a Google-owned platform that allows you to build conversational interfaces for various platforms, including Google Assistant, Facebook Messenger, and more. It has a visual interface and supports multiple programming languages, including Python.\n",
      "3. **Microsoft Bot Framework**: The Microsoft Bot Framework is a set of tools and services for building conversational AI applications. It supports multiple programming languages, including C#, Node.js, and Python.\n",
      "4. **Wit.ai**: Wit.ai is a Facebook-owned platform that allows you to build conversational interfaces for various platforms, including Facebook Messenger, Slack, and more. It has a visual interface and supports multiple programming languages, including Python.\n",
      "5. **Rive**: Rive is a conversational AI platform that allows you to build chatbots and voice assistants using a visual interface. It supports multiple programming languages, including Python.\n",
      "6. **Botpress**: Botpress is an open-source conversational AI platform that allows you to build chatbots and voice assistants using a visual interface. It supports multiple programming languages, including Python.\n",
      "7. **ManyChat**: ManyChat is a popular platform for building chatbots for messaging platforms like Facebook Messenger, WhatsApp, and more. It has a visual interface and supports multiple programming languages, including Python.\n",
      "8. **BotStar**: BotStar is a conversational AI platform that allows you to build chatbots and voice assistants using a visual interface. It supports multiple programming languages, including Python.\n",
      "9. **Zapier**: Zapier is an automation platform that allows you to build custom workflows and integrations using a visual interface. It supports multiple programming languages, including Python.\n",
      "10. **Microsoft Azure Bot Service**: The Microsoft Azure Bot Service is a cloud-based platform for building conversational AI applications. It supports multiple programming languages, including Python.\n",
      "\n",
      "These are just a few examples of the many libraries and frameworks available for building conversational AI applications. The choice of which one to use depends on your specific needs, the platforms you want to support, and your programming language of choice.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(create_faq_document(faq_questions, faq_answers))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
