{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34dfd9db-8f0f-4f90-8914-e1c86c8a396d",
   "metadata": {},
   "source": [
    "### Date: April 19, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a9cb68-4c93-4de1-836d-753ca766ebe1",
   "metadata": {},
   "source": [
    "![NVIDIA](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4772d59",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28013522",
   "metadata": {},
   "source": [
    "In this notebook we introduce agents which can reason about tool use and integrate the actual invocation of tools into LLM responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9ea06-814f-43fd-9f59-ce67dfcb1bbb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327550d4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75febe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc7aca5-1c4a-4cb0-bd28-d8a7636cc96d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e1244",
   "metadata": {},
   "source": [
    "## Create a Model Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61e3b445-73d1-4530-a9ee-df3ebe696c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://llama:8000/v1'\n",
    "model = 'meta/llama-3.1-8b-instruct'\n",
    "llm = ChatNVIDIA(base_url=base_url, model=model, temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe609bf-e066-45b0-9e33-d5b9959355c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ece7e40-53f5-4644-a656-8a3897c97b5b",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb370d-3815-4a89-8647-bd22fac66947",
   "metadata": {},
   "source": [
    "In the previous notebook we learned how to create tools, use LLMs to indicate when and how to invoke tools, and then, engineered a naive system of actually invoking tools when the LLM indicated we should.\n",
    "\n",
    "The naive approach we took lacked some very key features, perhaps the most important that we were unable to provide the LLM with the actual result of invoking its tool so that it could use the tool's result to inform its response to the user. To address this issue, and to facilitate many much more powerful capabilities, we tend to create **agents**.\n",
    "\n",
    "The [LangChain docs](https://python.langchain.com/v0.2/docs/concepts/#agents) have an excellent description of agents:\n",
    "> By themselves, language models can't take actions - they just output text. A big use case for LangChain is creating agents. Agents are systems that use an LLM as a reasoning engine to determine which actions to take and what the inputs to those actions should be. The results of those actions can then be fed back into the agent and it determines whether more actions are needed, or whether it is okay to finish.\n",
    "\n",
    "While the topic of agents is really big, and is deserving of its own deep study, we want to give some coverage to agents in this workshop, and are happy to give you a chance to use them, at least in a simple fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c80c77-26ec-4df9-8a78-6f172ce23d97",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07073c85-a893-43a0-b2eb-ee20e8f739d6",
   "metadata": {},
   "source": [
    "## A Tool for an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5704faf4-82c7-4200-a5f8-ba0b05aa6912",
   "metadata": {},
   "source": [
    "It should come as no surprise that before we can think about creating agents, we need to create tools that an agent might decide to use. We'll re-create here our simple multiplication tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bffdc66-45cc-48e0-9d70-f155560684ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiply(BaseModel):\n",
    "    \"\"\"Use when needed to get the product of multiplying two integers together.\"\"\"\n",
    "    a: int = Field(..., description=\"First integer to multiply.\")\n",
    "    b: int = Field(..., description=\"Second integer to multiply.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ca1364a-2448-4c57-b914-82a00fe0a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(args_schema=Multiply)\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    return a * b "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01dd593-4f06-45a6-8efa-cbcb1c81f0a4",
   "metadata": {},
   "source": [
    "Here we do a quick sanity check to make sure the multiply tool behaves as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fc3b88a-6425-40ab-945c-bf8dba100caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply.invoke({'a': 12, 'b': 10})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ce518-a846-435a-8a6c-8143f2ddba65",
   "metadata": {},
   "source": [
    "And we create a `tools` list containing our `multiply` tool for use later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd0663f0-3162-4bc1-a384-90a9a16cdadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [multiply] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8d1330-5a1c-46fd-b0da-d06b1e8e6db9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a36962-16f9-4b5b-9ee3-ff66ad14cdc1",
   "metadata": {},
   "source": [
    "## Creating a Simple Agent with LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932d37e0-2745-4a59-a600-facae95aad84",
   "metadata": {},
   "source": [
    "To create an agent capable of utilizing tools when appropriate, and integrating the result of a tool call into its response, we will use [LangGraph](https://langchain-ai.github.io/langgraph/).\n",
    "\n",
    "LangGraph is a spinoff open-source project from the folks who created LangChain. At a high level, LangGraph promotes the easy creation of graph-based workflows. Graphs are a collection of nodes, each responsible for doing some sort of computational work, and edges, which map between nodes and define when and how nodes will be invoked to perform work.\n",
    "\n",
    "We are going to limit our work with LangGraph here to using a simple, yet powerful, prebuilt agent that ships with LangGraph, but if you find yourself wanting to learn more about agent creation, we highly recommend taking the time to learn more about [LangGraph](https://langchain-ai.github.io/langgraph/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bd005d-566d-4ba9-a2f8-8056b7b4f3a1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f471008e-63c7-409e-8c51-03688a9a17cf",
   "metadata": {},
   "source": [
    "## Creating a Simple Agent with LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f021c40-e693-4516-9a46-74b3c21ff3eb",
   "metadata": {},
   "source": [
    "As we mentioned above, agent creation is a large topic worthy of its own coverage. However, LangGraph ships with some pre-built graphs that are very easy to use, and we are going to leverage one of them in this workshop, namely, a **ReAct** agent graph.\n",
    "\n",
    "**ReAct** stands for \"Reason and Act\" ([link to paper](https://arxiv.org/abs/2210.03629)). For this workshop we can think about the ReAct paradigm as a way of instructing an LLM to reason about whether or not it should use an external tool, or tools, and then when appropriate, utilize these tools before generating a final response.\n",
    "\n",
    "With LangGraph, to create a ReAct agent, we can simply import the `create_react_agent` function, and pass it an LLM instance (created above), and a list of tools (also created above) that the agent should have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e1c8bc8-eacc-4088-9cfb-3065699a2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20e712cd-984c-45db-a7a0-462440309f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(llm, tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f6af4d-f631-4097-8060-987fbf97fa81",
   "metadata": {},
   "source": [
    "We can use some helper methods on the agent to get a visual of how the agent graph is constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0afca8c7-4fa6-4962-b6ed-1a4fa168b071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c007478-7f8e-4532-8fdd-76eb684138b9",
   "metadata": {},
   "source": [
    "As mentioned above, the graph consists of nodes, capable of performing work, and edges which describe how data ought to be passed between nodes in order to accomplish work.\n",
    "\n",
    "In the above image we can see that at the start of the graph, incoming data will be routed to `agent` which in our case is our LLM instance which under the hood, by LangGraph, has been prompted to reason about whether or not to utilize any tools we provided to the agent. Depending on whether or not the LLM believes a tool should be utilized, the agent will either continue on to the `tools` node, where an appropriate tool will be invoked, then returning its return value back to the agent. Or, either if no tool needs calling, or all necessary tools have already been called, the agent node will send a final response to the end of the graph, where it can be viewed by the end user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c1e2f0-1f49-4537-902d-dd51016e2b36",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b277e4-f406-41fd-90a5-92ef74aeb009",
   "metadata": {},
   "source": [
    "## Invoking the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0af001-a2d0-44da-aa0a-8f60a435e290",
   "metadata": {},
   "source": [
    "LangGraph graphs are stateful, meaning, every graph has some state defined (usually a dict) that different parts of the graph can read from and write to.\n",
    "\n",
    "In the case of the pre-built `create_react_agent` graph we are using here, the graph's state has already been defined for us as a dict with a single `messages` key which itself contains a list of messages.\n",
    "\n",
    "What this means for us presently is that when we use the `agent` graph, we need to add our human message prompt to its state, namely, the `messages` property of a dict, and, that any other activity (such as AI messages) in the graph will also be added to this same `messages` property.\n",
    "\n",
    "Just like LangChain chains, our LangGraph agent has `invoke`, `batch` and `stream` methods on it. To begin, let's use the `invoke` method on the graph using a simple prompt, and one where we would not expect the agent to believe it requires the use of the `multiply` tool we have provided.\n",
    "\n",
    "The return value of invoking the graph will be the state of the agent graph after it completes, which we'll store in a variable for further exploration below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6c92ea3-4ddf-4dec-a966-6f9a30a63bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_state = agent.invoke({\"messages\": [\"Give a short summary of directed cyclical graphs in the context of computer science.\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e809bd2-695f-4dba-a4d3-363c49fc3268",
   "metadata": {},
   "source": [
    "`agent_state` will be the current state of the graph, and looking at it here we see that it is a dict with a `messages` key comprised of all the messages sent to and generated by the graph. In our case we have the `HumanMessage` that we sent into the graph, as well as the `AIMessage` response from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dec43c15-eabb-4cb8-aa75-d6045d763710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Give a short summary of directed cyclical graphs in the context of computer science.', id='06ecb695-0075-4f9e-8504-492e5288c764'), AIMessage(content='Directed cyclical graphs are a type of graph in computer science where edges have direction and there is at least one path that starts and ends at the same vertex, forming a cycle. This means that it is possible to traverse the graph in a loop, visiting the same vertex multiple times. Directed cyclical graphs are commonly used to model real-world systems where there are relationships between objects that have direction, such as social networks, traffic flow, or computer networks. They are also used in algorithms for tasks such as topological sorting, finding strongly connected components, and finding the shortest path in a graph.', response_metadata={'role': 'assistant', 'content': 'Directed cyclical graphs are a type of graph in computer science where edges have direction and there is at least one path that starts and ends at the same vertex, forming a cycle. This means that it is possible to traverse the graph in a loop, visiting the same vertex multiple times. Directed cyclical graphs are commonly used to model real-world systems where there are relationships between objects that have direction, such as social networks, traffic flow, or computer networks. They are also used in algorithms for tasks such as topological sorting, finding strongly connected components, and finding the shortest path in a graph.', 'token_usage': {'prompt_tokens': 262, 'total_tokens': 382, 'completion_tokens': 120}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'}, id='run-5d8c6b16-70ee-4b5d-9148-a1cb234eacef-0', role='assistant')]}\n"
     ]
    }
   ],
   "source": [
    "print(agent_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824b3ddb-59ba-44ba-ab3c-97c78d1f9593",
   "metadata": {},
   "source": [
    "As a convenience, LangGraph messages have a `pretty_print` method. Let's loop over the messages and use this helper method to get a clearer read out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f12857f2-acaa-44ec-992b-8e67c713e867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Give a short summary of directed cyclical graphs in the context of computer science.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Directed cyclical graphs are a type of graph in computer science where edges have direction and there is at least one path that starts and ends at the same vertex, forming a cycle. This means that it is possible to traverse the graph in a loop, visiting the same vertex multiple times. Directed cyclical graphs are commonly used to model real-world systems where there are relationships between objects that have direction, such as social networks, traffic flow, or computer networks. They are also used in algorithms for tasks such as topological sorting, finding strongly connected components, and finding the shortest path in a graph.\n"
     ]
    }
   ],
   "source": [
    "for message in agent_state['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb81c940-5d40-42f4-a505-32682d75c114",
   "metadata": {},
   "source": [
    "Here we see more plainly the human message we passed into the agent graph, and the AI message response it generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2098c41-9f65-492d-a2f1-58fc57f1a231",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8c8c94-5fb0-412c-9c5c-4d79c1d9e8c7",
   "metadata": {},
   "source": [
    "## Invoking the Agent to Use a Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f253dc0-7b8a-4094-b34e-4be59208d7d0",
   "metadata": {},
   "source": [
    "Next let's invoke our agent graph, but this time with a prompt where we would expect the agent to use the `multiply` tool we have provided it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4990cfc-4e43-411f-802e-1d7c74fdda84",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_state = agent.invoke({\"messages\": [\"What is 19944 times 2342?\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420769f7-d18f-43e0-b731-de926f5e3319",
   "metadata": {},
   "source": [
    "Let's again loop over the messages in the returned state, using the `pretty_print` helper for a clean view of all the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c51c110f-3343-48b4-bd19-a44888ed0a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is 19944 times 2342?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (chatcmpl-tool-9b00a68e333b4bea81580eaf144cb51f)\n",
      " Call ID: chatcmpl-tool-9b00a68e333b4bea81580eaf144cb51f\n",
      "  Args:\n",
      "    a: 19944\n",
      "    b: 2342\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "46708848\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The product of 19944 and 2342 is 46708848.\n"
     ]
    }
   ],
   "source": [
    "for message in agent_state['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94f33c-ba36-4dea-8691-85f3d459a2d5",
   "metadata": {},
   "source": [
    "Here we see our initial human message, but now, as we might expect from what we learned in the previous notebook, rather than the AI message returning a response, it instead indicates the need for a tool call. After the AI message we see a new kind of message, a tool message, which is a message from the tool (in this case `multiply`) along with the value it returned after being invoked. Finally, we have another AI message which generates a response to the initial human message, using the result of the tool call in its response.\n",
    "\n",
    "Thus we observe how agents truly unlock the capabilities of tool creation. The agent is itself able to reason about when a tool ought to be used, is able to actually call the tool, and then is able to construct a meaningful response back to the user using the value of the resulting tool call.\n",
    "\n",
    "Let's try one more prompt, but this time using a prompt that ought to require that the `multiply` tool be used more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4eef29c-ae47-49a7-8198-6ffc42b898fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_state = agent.invoke({\"messages\": [\"What is 19944 times 2342? Also, what is 9877 time 22875?\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b51a5d-915e-4980-a89a-32b75f019aef",
   "metadata": {},
   "source": [
    "Let's again loop over the messages to see what occured within the agent graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "980fe1b3-6abc-46b8-ae5a-774a57bfe502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is 19944 times 2342? Also, what is 9877 time 22875?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (chatcmpl-tool-712d8c34d4de4b56ad533cc4cd287c35)\n",
      " Call ID: chatcmpl-tool-712d8c34d4de4b56ad533cc4cd287c35\n",
      "  Args:\n",
      "    a: 19944\n",
      "    b: 2342\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "46708848\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (chatcmpl-tool-b0794187071e4d71825fb82939b05d12)\n",
      " Call ID: chatcmpl-tool-b0794187071e4d71825fb82939b05d12\n",
      "  Args:\n",
      "    a: 9877\n",
      "    b: 22875\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "225936375\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The product of 19944 and 2342 is 46,708,848. The product of 9877 and 22875 is 225,936,375.\n"
     ]
    }
   ],
   "source": [
    "for message in agent_state['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ee1c47-5da1-4935-b6ba-c157f924c603",
   "metadata": {},
   "source": [
    "Given the prompt which asked about 2 different multiplication problems, the agent was successfully able to make 2 separate tool calls and only after making them both, synthesize the results of both tool calls into its final response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9789d8c3-0dea-4a89-95ce-e4b8f45e3a92",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbe8f52-2df1-4d14-9930-f5de66edb254",
   "metadata": {},
   "source": [
    "## Inadvertent Tool Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99185ba0-ce6f-4fd0-9d19-03b573c7d082",
   "metadata": {},
   "source": [
    "As an experiment, let's send another message into our agent where we would not expect the agent to use the `multiply` tool we have provided it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3846bb59-4667-47f3-908b-56bf28433152",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_state = agent.invoke({\"messages\": [\"In what year was NVIDIA founded?\"]}) # The actual answer is 1993"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a84eb350-d436-4ae5-96ab-7137304add9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "In what year was NVIDIA founded?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (chatcmpl-tool-e506206b95a0487e89abfb408d8c12c6)\n",
      " Call ID: chatcmpl-tool-e506206b95a0487e89abfb408d8c12c6\n",
      "  Args:\n",
      "    a: 1958\n",
      "    b: 1\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "1958\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "NVIDIA was founded in 1958.\n"
     ]
    }
   ],
   "source": [
    "for message in agent_state['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a536d6-5abf-4b31-8a91-680eaa20ebd6",
   "metadata": {},
   "source": [
    "Unfortunately, the agent thought that it ought to use the `multiply` tool to answer this question. It's not entirely clear why, but it seems like a reasonable hypothesis that because the answer to the question was an integer, the agent thought it ought to use the tool.\n",
    "\n",
    "Even worse, the LLM hallucinated two integer values to pass into the `multiply` tool, neither of which were the actual integer value of the year when NVIDIA was founded (which is 1993), and then used the result of this mistaken multiplication in its response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2419e8-e6ee-4e68-b299-c5e12e2e9dc2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9894e70c-9f97-448c-9983-0839f813f245",
   "metadata": {},
   "source": [
    "## Prompt Engineer Better Tool Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282a5106-362a-4b2f-ba6e-28530d328d38",
   "metadata": {},
   "source": [
    "At this point in the workshop we already know the importance of specific prompting. So let's try to be more specific in our prompt in order to get the LLM agent to behave as we would like.\n",
    "\n",
    "If we wanted we could try being more specific about when the agent ought to be using its tools by writing a longer more specific prompt, but since we are trying to impact the overarching behavior of the model, it might make more sense for us to try including a system message.\n",
    "\n",
    "Let's try the following which is quite explicit about the behavior we would like, and utilizes zero-shot COT prompting (in the form of \"Think hard about...\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62c385ac-1cc9-428c-b826-8c965c3e29d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\\\n",
    "You are a helpful assistant capable of tool calling when helpful, necessary, and appropriate.\n",
    "\n",
    "Think hard about whether or not you need to call a tool, \\\n",
    "based on your tools' descriptions and use them, but only when appropriate!\n",
    "\n",
    "Whether or not you need to call a tool, address the user's query in a helpful informative way.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7c336a-fb30-4c8f-862d-3ffec3be94ac",
   "metadata": {},
   "source": [
    "In the case of the pre-built agent we are using, we can add a system message when instantiating it using the `state_modifier` named argument. Here's the docstring for `create_react_agent`. You don't need to read the whole thing, but take a look at the use of the `state_modifier` argument which we can use to include a system message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccfc0061-2a1c-47a0-a1c8-4763ba22e1cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function create_react_agent in module langgraph.prebuilt.chat_agent_executor:\n",
      "\n",
      "create_react_agent(model: langchain_core.runnables.base.Runnable[typing.Union[langchain_core.prompt_values.PromptValue, str, typing.Sequence[typing.Union[langchain_core.messages.base.BaseMessage, typing.List[str], typing.Tuple[str, str], str, typing.Dict[str, typing.Any]]]], typing.Union[langchain_core.messages.base.BaseMessage, str]], tools: Union[langgraph.prebuilt.tool_executor.ToolExecutor, Sequence[langchain_core.tools.base.BaseTool], langgraph.prebuilt.tool_node.ToolNode], *, state_schema: Optional[Type[~StateSchema]] = None, messages_modifier: Union[langchain_core.messages.system.SystemMessage, str, Callable[[Sequence[langchain_core.messages.base.BaseMessage]], Sequence[langchain_core.messages.base.BaseMessage]], langchain_core.runnables.base.Runnable[Sequence[langchain_core.messages.base.BaseMessage], Sequence[langchain_core.messages.base.BaseMessage]], NoneType] = None, state_modifier: Union[langchain_core.messages.system.SystemMessage, str, Callable[[~StateSchema], Sequence[langchain_core.messages.base.BaseMessage]], langchain_core.runnables.base.Runnable[~StateSchema, Sequence[langchain_core.messages.base.BaseMessage]], NoneType] = None, checkpointer: Optional[langgraph.checkpoint.base.BaseCheckpointSaver] = None, interrupt_before: Optional[Sequence[str]] = None, interrupt_after: Optional[Sequence[str]] = None, debug: bool = False) -> langgraph.graph.graph.CompiledGraph\n",
      "    Creates a graph that works with a chat model that utilizes tool calling.\n",
      "\n",
      "    Args:\n",
      "        model: The `LangChain` chat model that supports tool calling.\n",
      "        tools: A list of tools, a ToolExecutor, or a ToolNode instance.\n",
      "        state_schema: An optional state schema that defines graph state.\n",
      "            Must have `messages` and `is_last_step` keys.\n",
      "            Defaults to `AgentState` that defines those two keys.\n",
      "        messages_modifier: An optional\n",
      "            messages modifier. This applies to messages BEFORE they are passed into the LLM.\n",
      "\n",
      "            Can take a few different forms:\n",
      "\n",
      "            - SystemMessage: this is added to the beginning of the list of messages.\n",
      "            - str: This is converted to a SystemMessage and added to the beginning of the list of messages.\n",
      "            - Callable: This function should take in a list of messages and the output is then passed to the language model.\n",
      "            - Runnable: This runnable should take in a list of messages and the output is then passed to the language model.\n",
      "            !!! Warning\n",
      "                `messages_modifier` parameter is deprecated as of version 0.1.9 and will be removed in 0.2.0\n",
      "        state_modifier: An optional\n",
      "            state modifier. This takes full graph state BEFORE the LLM is called and prepares the input to LLM.\n",
      "\n",
      "            Can take a few different forms:\n",
      "\n",
      "            - SystemMessage: this is added to the beginning of the list of messages in state[\"messages\"].\n",
      "            - str: This is converted to a SystemMessage and added to the beginning of the list of messages in state[\"messages\"].\n",
      "            - Callable: This function should take in full graph state and the output is then passed to the language model.\n",
      "            - Runnable: This runnable should take in full graph state and the output is then passed to the language model.\n",
      "        checkpointer: An optional checkpoint saver object. This is useful for persisting\n",
      "            the state of the graph (e.g., as chat memory).\n",
      "        interrupt_before: An optional list of node names to interrupt before.\n",
      "            Should be one of the following: \"agent\", \"tools\".\n",
      "            This is useful if you want to add a user confirmation or other interrupt before taking an action.\n",
      "        interrupt_after: An optional list of node names to interrupt after.\n",
      "            Should be one of the following: \"agent\", \"tools\".\n",
      "            This is useful if you want to return directly or run additional processing on an output.\n",
      "        debug: A flag indicating whether to enable debug mode.\n",
      "\n",
      "    Returns:\n",
      "        A compiled LangChain runnable that can be used for chat interactions.\n",
      "\n",
      "    The resulting graph looks like this:\n",
      "\n",
      "    ``` mermaid\n",
      "    stateDiagram-v2\n",
      "        [*] --> Start\n",
      "        Start --> Agent\n",
      "        Agent --> Tools : continue\n",
      "        Tools --> Agent\n",
      "        Agent --> End : end\n",
      "        End --> [*]\n",
      "\n",
      "        classDef startClass fill:#ffdfba;\n",
      "        classDef endClass fill:#baffc9;\n",
      "        classDef otherClass fill:#fad7de;\n",
      "\n",
      "        class Start startClass\n",
      "        class End endClass\n",
      "        class Agent,Tools otherClass\n",
      "    ```\n",
      "\n",
      "    The \"agent\" node calls the language model with the messages list (after applying the messages modifier).\n",
      "    If the resulting AIMessage contains `tool_calls`, the graph will then call the [\"tools\"][toolnode].\n",
      "    The \"tools\" node executes the tools (1 tool per `tool_call`) and adds the responses to the messages list\n",
      "    as `ToolMessage` objects. The agent node then calls the language model again.\n",
      "    The process repeats until no more `tool_calls` are present in the response.\n",
      "    The agent then returns the full list of messages as a dictionary containing the key \"messages\".\n",
      "\n",
      "    ``` mermaid\n",
      "        sequenceDiagram\n",
      "            participant U as User\n",
      "            participant A as Agent (LLM)\n",
      "            participant T as Tools\n",
      "            U->>A: Initial input\n",
      "            Note over A: Messages modifier + LLM\n",
      "            loop while tool_calls present\n",
      "                A->>T: Execute tools\n",
      "                T-->>A: ToolMessage for each tool_calls\n",
      "            end\n",
      "            A->>U: Return final state\n",
      "    ```\n",
      "\n",
      "    Examples:\n",
      "        Use with a simple tool:\n",
      "\n",
      "        ```pycon\n",
      "        >>> from datetime import datetime\n",
      "        >>> from langchain_core.tools import tool\n",
      "        >>> from langchain_openai import ChatOpenAI\n",
      "        >>> from langgraph.prebuilt import create_react_agent\n",
      "        >>>\n",
      "        >>> @tool\n",
      "        ... def check_weather(location: str, at_time: datetime | None = None) -> float:\n",
      "        ...     '''Return the weather forecast for the specified location.'''\n",
      "        ...     return f\"It's always sunny in {location}\"\n",
      "        >>>\n",
      "        >>> tools = [check_weather]\n",
      "        >>> model = ChatOpenAI(model=\"gpt-4o\")\n",
      "        >>> graph = create_react_agent(model, tools=tools)\n",
      "        >>> inputs = {\"messages\": [(\"user\", \"what is the weather in sf\")]}\n",
      "        >>> for s in graph.stream(inputs, stream_mode=\"values\"):\n",
      "        ...     message = s[\"messages\"][-1]\n",
      "        ...     if isinstance(message, tuple):\n",
      "        ...         print(message)\n",
      "        ...     else:\n",
      "        ...         message.pretty_print()\n",
      "        ('user', 'what is the weather in sf')\n",
      "        ================================== Ai Message ==================================\n",
      "        Tool Calls:\n",
      "        check_weather (call_LUzFvKJRuaWQPeXvBOzwhQOu)\n",
      "        Call ID: call_LUzFvKJRuaWQPeXvBOzwhQOu\n",
      "        Args:\n",
      "            location: San Francisco\n",
      "        ================================= Tool Message =================================\n",
      "        Name: check_weather\n",
      "        It's always sunny in San Francisco\n",
      "        ================================== Ai Message ==================================\n",
      "        The weather in San Francisco is sunny.\n",
      "        ```\n",
      "        Add a system prompt for the LLM:\n",
      "\n",
      "        ```pycon\n",
      "        >>> system_prompt = \"You are a helpful bot named Fred.\"\n",
      "        >>> graph = create_react_agent(model, tools, state_modifier=system_prompt)\n",
      "        >>> inputs = {\"messages\": [(\"user\", \"What's your name? And what's the weather in SF?\")]}\n",
      "        >>> for s in graph.stream(inputs, stream_mode=\"values\"):\n",
      "        ...     message = s[\"messages\"][-1]\n",
      "        ...     if isinstance(message, tuple):\n",
      "        ...         print(message)\n",
      "        ...     else:\n",
      "        ...         message.pretty_print()\n",
      "        ('user', \"What's your name? And what's the weather in SF?\")\n",
      "        ================================== Ai Message ==================================\n",
      "        Hi, my name is Fred. Let me check the weather in San Francisco for you.\n",
      "        Tool Calls:\n",
      "        check_weather (call_lqhj4O0hXYkW9eknB4S41EXk)\n",
      "        Call ID: call_lqhj4O0hXYkW9eknB4S41EXk\n",
      "        Args:\n",
      "            location: San Francisco\n",
      "        ================================= Tool Message =================================\n",
      "        Name: check_weather\n",
      "        It's always sunny in San Francisco\n",
      "        ================================== Ai Message ==================================\n",
      "        The weather in San Francisco is currently sunny. If you need any more details or have other questions, feel free to ask!\n",
      "        ```\n",
      "\n",
      "        Add a more complex prompt for the LLM:\n",
      "\n",
      "        ```pycon\n",
      "        >>> from langchain_core.prompts import ChatPromptTemplate\n",
      "        >>> prompt = ChatPromptTemplate.from_messages([\n",
      "        ...     (\"system\", \"You are a helpful bot named Fred.\"),\n",
      "        ...     (\"placeholder\", \"{messages}\"),\n",
      "        ...     (\"user\", \"Remember, always be polite!\"),\n",
      "        ... ])\n",
      "        >>> def modify_state_messages(state: AgentState):\n",
      "        ...     # You can do more complex modifications here\n",
      "        ...     return prompt.invoke({\"messages\": state[\"messages\"]})\n",
      "        >>>\n",
      "        >>> graph = create_react_agent(model, tools, state_modifier=modify_state_messages)\n",
      "        >>> inputs = {\"messages\": [(\"user\", \"What's your name? And what's the weather in SF?\")]}\n",
      "        >>> for s in graph.stream(inputs, stream_mode=\"values\"):\n",
      "        ...     message = s[\"messages\"][-1]\n",
      "        ...     if isinstance(message, tuple):\n",
      "        ...         print(message)\n",
      "        ...     else:\n",
      "        ...         message.pretty_print()\n",
      "        ```\n",
      "\n",
      "        Add complex prompt with custom graph state:\n",
      "\n",
      "        ```pycon\n",
      "        >>> from typing import TypedDict\n",
      "        >>> prompt = ChatPromptTemplate.from_messages(\n",
      "        ...     [\n",
      "        ...         (\"system\", \"Today is {today}\"),\n",
      "        ...         (\"placeholder\", \"{messages}\"),\n",
      "        ...     ]\n",
      "        ... )\n",
      "        >>>\n",
      "        >>> class CustomState(TypedDict):\n",
      "        ...     today: str\n",
      "        ...     messages: Annotated[list[BaseMessage], add_messages]\n",
      "        ...     is_last_step: str\n",
      "        >>>\n",
      "        >>> graph = create_react_agent(model, tools, state_schema=CustomState, state_modifier=prompt)\n",
      "        >>> inputs = {\"messages\": [(\"user\", \"What's today's date? And what's the weather in SF?\")], \"today\": \"July 16, 2004\"}\n",
      "        >>> for s in graph.stream(inputs, stream_mode=\"values\"):\n",
      "        ...     message = s[\"messages\"][-1]\n",
      "        ...     if isinstance(message, tuple):\n",
      "        ...         print(message)\n",
      "        ...     else:\n",
      "        ...         message.pretty_print()\n",
      "        ```\n",
      "\n",
      "        Add \"chat memory\" to the graph:\n",
      "\n",
      "        ```pycon\n",
      "        >>> from langgraph.checkpoint.memory import MemorySaver\n",
      "        >>> graph = create_react_agent(model, tools, checkpointer=MemorySaver())\n",
      "        >>> config = {\"configurable\": {\"thread_id\": \"thread-1\"}}\n",
      "        >>> def print_stream(graph, inputs, config):\n",
      "        ...     for s in graph.stream(inputs, config, stream_mode=\"values\"):\n",
      "        ...         message = s[\"messages\"][-1]\n",
      "        ...         if isinstance(message, tuple):\n",
      "        ...             print(message)\n",
      "        ...         else:\n",
      "        ...             message.pretty_print()\n",
      "        >>> inputs = {\"messages\": [(\"user\", \"What's the weather in SF?\")]}\n",
      "        >>> print_stream(graph, inputs, config)\n",
      "        >>> inputs2 = {\"messages\": [(\"user\", \"Cool, so then should i go biking today?\")]}\n",
      "        >>> print_stream(graph, inputs2, config)\n",
      "        ('user', \"What's the weather in SF?\")\n",
      "        ================================== Ai Message ==================================\n",
      "        Tool Calls:\n",
      "        check_weather (call_ChndaktJxpr6EMPEB5JfOFYc)\n",
      "        Call ID: call_ChndaktJxpr6EMPEB5JfOFYc\n",
      "        Args:\n",
      "            location: San Francisco\n",
      "        ================================= Tool Message =================================\n",
      "        Name: check_weather\n",
      "        It's always sunny in San Francisco\n",
      "        ================================== Ai Message ==================================\n",
      "        The weather in San Francisco is sunny. Enjoy your day!\n",
      "        ================================ Human Message =================================\n",
      "        Cool, so then should i go biking today?\n",
      "        ================================== Ai Message ==================================\n",
      "        Since the weather in San Francisco is sunny, it sounds like a great day for biking! Enjoy your ride!\n",
      "        ```\n",
      "\n",
      "        Add an interrupt to let the user confirm before taking an action:\n",
      "\n",
      "        ```pycon\n",
      "        >>> graph = create_react_agent(\n",
      "        ...     model, tools, interrupt_before=[\"tools\"], checkpointer=MemorySaver()\n",
      "        >>> )\n",
      "        >>> config = {\"configurable\": {\"thread_id\": \"thread-1\"}}\n",
      "        >>> def print_stream(graph, inputs, config):\n",
      "        ...     for s in graph.stream(inputs, config, stream_mode=\"values\"):\n",
      "        ...         message = s[\"messages\"][-1]\n",
      "        ...         if isinstance(message, tuple):\n",
      "        ...             print(message)\n",
      "        ...         else:\n",
      "        ...             message.pretty_print()\n",
      "\n",
      "        >>> inputs = {\"messages\": [(\"user\", \"What's the weather in SF?\")]}\n",
      "        >>> print_stream(graph, inputs, config)\n",
      "        >>> snapshot = graph.get_state(config)\n",
      "        >>> print(\"Next step: \", snapshot.next)\n",
      "        >>> print_stream(graph, None, config)\n",
      "        ```\n",
      "\n",
      "        Add a timeout for a given step:\n",
      "\n",
      "        ```pycon\n",
      "        >>> import time\n",
      "        >>> @tool\n",
      "        ... def check_weather(location: str, at_time: datetime | None = None) -> float:\n",
      "        ...     '''Return the weather forecast for the specified location.'''\n",
      "        ...     time.sleep(2)\n",
      "        ...     return f\"It's always sunny in {location}\"\n",
      "        >>>\n",
      "        >>> tools = [check_weather]\n",
      "        >>> graph = create_react_agent(model, tools)\n",
      "        >>> graph.step_timeout = 1 # Seconds\n",
      "        >>> for s in graph.stream({\"messages\": [(\"user\", \"what is the weather in sf\")]}):\n",
      "        ...     print(s)\n",
      "        TimeoutError: Timed out at step 2\n",
      "        ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(create_react_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3ae76c-556f-4f5d-b331-b94d2d168cbb",
   "metadata": {},
   "source": [
    "With that in mind, we'll recreate our `agent` instance, but this time passing in the system message we drafted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ffc8399-b463-427f-a671-1eb6f94cd843",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(llm, tools=tools, state_modifier=system_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19bd86f-afda-46ff-96f7-79c36675a851",
   "metadata": {},
   "source": [
    "Let's invoke our new agent with the same prompt that was giving us trouble above to see if the inclusion of the system message impacted its behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "171445fe-18c0-4ec2-926d-46f37715604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_state = agent.invoke({\"messages\": ['In what year was NVIDIA founded?']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd95e7cf-17e1-450f-9d62-33037ea051a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "In what year was NVIDIA founded?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The NVIDIA Corporation was founded in 1993.\n"
     ]
    }
   ],
   "source": [
    "for message in agent_state['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc1bef8-b628-4619-a368-0f92cb8885b2",
   "metadata": {},
   "source": [
    "That's the answer we wanted.\n",
    "\n",
    "Since we've made significant updates to our prompt in a way that impacts how and when it calls tools, let's make sure that the agent is still invoking the `multiply` tool as we would wish when given a multiplication problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9c639cd-bd82-4444-81cf-c8ac07d1790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_state = agent.invoke({\"messages\": ['What is 87889 times 23484?']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d901f239-b15d-4de1-9079-a704c6a4cc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is 87889 times 23484?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (chatcmpl-tool-f041d84419164a2595b070ae1a54ab17)\n",
      " Call ID: chatcmpl-tool-f041d84419164a2595b070ae1a54ab17\n",
      "  Args:\n",
      "    a: 87889\n",
      "    b: 23484\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "2063985276\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The product of 87889 and 23484 is 2063985276.\n"
     ]
    }
   ],
   "source": [
    "for message in agent_state['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e775d8e-1ae1-4587-a2cb-c41a66311f79",
   "metadata": {},
   "source": [
    "Given a prompt requesting two numbers to be multiplied, the agent correctly utilized the `multiply` tool and used its result in its final response, just as we would hope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e6762a-6478-4e5f-85be-94db62f7dc2c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1366f48-1d8e-4301-95a8-3d7e1a09f5a8",
   "metadata": {},
   "source": [
    "## Creating a Chain for Agent Invocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9df071-153b-46f1-a3be-876dfd752cae",
   "metadata": {},
   "source": [
    "As it stands, we have the ability to invoke our ReAct agent and observe all the messages in its final state. Let's go one step further and construct a chain to make invoking the agent more straightforward, and also, make it easier to observe only the final response we get back from the agent, potentially after the agent has utilized any tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8460abf8-3069-40dd-afdd-1911b24a4cb3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7157f8d-7543-4692-a3fe-34c359dd71e8",
   "metadata": {},
   "source": [
    "## Simplify Passing the Agent a Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15bd3c8-816d-41f8-9edc-1c0d06729440",
   "metadata": {},
   "source": [
    "Let's start by creating a `RunnableLambda` that expects our prompt and returns a dict with a `messages` key in the format expected by our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e4998bb-ada4-4491-9891-c155e6ae552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_agent_state = RunnableLambda(lambda prompt: {'messages': [prompt]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd1ba5c-380d-4194-8c2c-58aeb4f897f6",
   "metadata": {},
   "source": [
    "Let's invoke `convert_to_agent_state` with a string prompt to make sure it converts our string prompt into the format expected by our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "696ea6a8-5e24-4068-9175-fc88bba00a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': ['In what year was NVIDIA founded?']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_to_agent_state.invoke('In what year was NVIDIA founded?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe5c3b2-2032-4eff-8d67-ff66fc3db259",
   "metadata": {},
   "source": [
    "The result is a dict with a `messages` key containing a list of messages (in this case one message), which is exactly the format we need to send into the agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2ef77a-ba4f-4c4e-9740-099bddd95340",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e672570-b44a-4e74-a62f-47a3a9a38e1f",
   "metadata": {},
   "source": [
    "## Create a Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7237b3-1168-4478-a636-528de6a0ab9a",
   "metadata": {},
   "source": [
    "Let's create a simple chain using the custom runnable we just created, and the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0c13ad5-2804-4d45-8ddb-33aa38754226",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = convert_to_agent_state | agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f789e56c-d5bb-420e-aa08-2034223e53d0",
   "metadata": {},
   "source": [
    "We should be able to invoke this chain with a string prompt and get back the final agent state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5460ac87-8940-4fc2-b279-8a56a4a80c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_state = chain.invoke('In what year was NVIDIA founded?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "283101d6-24b1-495b-b64d-b121e5032443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "In what year was NVIDIA founded?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The NVIDIA Corporation was founded in 1993.\n"
     ]
    }
   ],
   "source": [
    "for message in agent_state['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62ea958-0bf8-4627-bd34-116e6c726539",
   "metadata": {},
   "source": [
    "So far everything appears to be working as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db508d77-f9ad-45c6-9168-7d1db923f388",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1059dbf-e55f-49eb-a44e-0a13a97e64a7",
   "metadata": {},
   "source": [
    "## Simplify Viewing the Final Message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f71f47-b003-4acf-b2de-eeae09b92cad",
   "metadata": {},
   "source": [
    "Finally let's create another `RunnableLambda`, but this time to take the final agent state, which we know from earlier observations contains several messages, and return only the `content` property of its final message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb78392e-4819-4ff1-8cbd-c3aab08ff299",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_state_parser = RunnableLambda(lambda final_agent_state: final_agent_state['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67bc8c1-911e-4e39-9d30-e20df255b776",
   "metadata": {},
   "source": [
    "If we create now a new chain using all the constituent parts, we should be able to invoke it with a string prompt and get back a string response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15d31b7d-02e9-4709-9c95-ae87edcbaa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = convert_to_agent_state | agent | agent_state_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22acd03e-cb28-4743-a6f7-3d178d137b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The NVIDIA Corporation was founded in 1993.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('In what year was NVIDIA founded?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f369ffae-5343-437c-ad52-c65d6928bca0",
   "metadata": {},
   "source": [
    "This looks great.\n",
    "\n",
    "Let's also make sure our chain is handling multiplication problems as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33da17be-e1b7-4ab2-aa5d-4ec6b6ba5df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The product of 19944 and 2342 is 46708848.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What is 19944 times 2342?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b80e6b9-e4aa-40a4-98dd-7bb738d72063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46708848"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "19944*2342 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d2bfba-d917-4e6c-af7c-6477a0cf98b2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945066a0-acda-4d0e-9ad7-93eeb6cfe1b1",
   "metadata": {},
   "source": [
    "## Exercise: Create Air Quality Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328c7987-9e79-47c3-b91a-24eb88002dbb",
   "metadata": {},
   "source": [
    "For this exercise you will create an agent that is capable of using an external API to fetch real-time air quality information for a given location.\n",
    "\n",
    "To assist your work, we've provided the following function that given latitude and longitude coordinates will return the current air quality, as a categorical string, for that location.\n",
    "\n",
    "You don't need to concern yourself too much with the inner workings of this function. Just know that it uses the free open source weather API Open Meteo to fetch results and then converts the numerical results retrieved from the API into a categorical string like \"Good\", \"Fair\", \"Poor\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4dae0daf-4cfe-4d37-a692-00ca34759614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_air_quality_category_for_location(latitude: float, longitude: float) -> str:\n",
    "    base_url = \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"hourly\": \"european_aqi\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if \"hourly\" in data:\n",
    "            euro_aqi = data['hourly']['european_aqi'][0]\n",
    "            \n",
    "            # Determine AQI category\n",
    "            if euro_aqi <= 20:\n",
    "                return \"Good\"\n",
    "            elif euro_aqi <= 40:\n",
    "                return \"Fair\"\n",
    "            elif euro_aqi <= 60:\n",
    "                return \"Moderate\"\n",
    "            elif euro_aqi <= 80:\n",
    "                return \"Poor\"\n",
    "            elif euro_aqi <= 100:\n",
    "                return \"Very Poor\"\n",
    "            else:\n",
    "                return \"Extremely Poor\"\n",
    "        else:\n",
    "            return \"No air quality data found for the given coordinates.\"\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645a4f92-da16-4ab6-9702-f999d77020db",
   "metadata": {},
   "source": [
    "It's worth mentioning, that the LLM we are using today is quite capable, without any external tool use, of converting a string location into longitude and latitude coordinates, so we don't need an additional tool to convert a location name provided in a prompt into the latitude and longitude coordinates that `get_air_quality_category_for_location` requires.\n",
    "\n",
    "Just to prove the point, here we has the LLM to give us the coordinates for Mumbai, India in the floating point format that `get_air_quality_category_for_location` expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8584b1f5-ea2c-4ebb-84f3-6cdc74c43723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latitude and longitude coordinates for Mumbai, India are:\n",
      "\n",
      "Latitude: 19.0760 N\n",
      "Longitude: 72.8777 E\n",
      "\n",
      "As floating point numbers, these coordinates are:\n",
      "\n",
      "Latitude: 19.0760\n",
      "Longitude: 72.8777\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"Give me the latitude and longitude coordinates for Mumbai, India as floating point numbers.\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226daf41-c6f9-4b47-a6f0-1d2b307bb0a1",
   "metadata": {},
   "source": [
    "With thse coordinates we can now demonstrate how the `get_air_quality_category_for_location` is able to retrieve real-time air-quality information about the location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d91302f-e539-4713-b690-ebb84eb53162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Poor'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_air_quality_category_for_location(19.0760, 72.8777)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1873f8a6-0d0b-40de-9f96-eb17f634a607",
   "metadata": {},
   "source": [
    "To complete this exercise, you'll need to do the following:\n",
    "- Create a tool out of the provided `get_air_quality_category_for_location` function.\n",
    "- Create an agent (using `create_react_agent`) that can utilize the tool you created and respond to the user.\n",
    "- Create a chain utilizing your agent that will expect a string prompt and return a string response from the agent.\n",
    "\n",
    "By the time you sucessfully complete your chain, you should be able to batch send it the following prompts, getting back appropriate responses to them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "094c25d2-0c27-44b1-b315-1a28a2d0576a",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_agent_test_prompts = [\n",
    "    \"What is the current air quality in Korobosea in Papua New Guinea?\",\n",
    "    \"What is the current air quality in Washington DC?\",\n",
    "    \"What is the current air quality in Mumbai?\",\n",
    "    \"Where is the city of Rome located?\" # Make sure agent behaves as expected when not needing to make a tool call.\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b323f3dc-b9a4-4f9e-bb17-65e2db65f270",
   "metadata": {},
   "source": [
    "If you're up for the challenge, feel free to jump right in. If you prefer, expand the _Walkthrough_ section below for step-by-step guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a0e017-3900-4aba-9c52-72b3cd2b842e",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc0fabd-0531-4340-b588-9d280400cb64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9da166e3-424c-45c1-ac70-9ed7dcb9ef2d",
   "metadata": {},
   "source": [
    "## Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98732661-a93a-4a2c-bc6f-cc66b704da6c",
   "metadata": {},
   "source": [
    "### Create Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feef0b9c-cff6-43d3-bd5d-eeb8217e921f",
   "metadata": {},
   "source": [
    "As a first step, convert the `get_air_quality_category_for_location` function into a tool.\n",
    "\n",
    "Feel free to check out the *Solution* below if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82711b73-b184-49e9-befe-a173b23829af",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eefca30-2b7f-47be-ad93-16a693088e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2e1d583-a2fa-4b66-928f-cd8a30d0d4cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b7462-28ff-4f20-99d9-180c402940c4",
   "metadata": {},
   "source": [
    "As we've done previously, we create a tool first by using a Pydantic class that has a docstring describing what the tool does and `Field`s that describe each of the expected arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7104cb80-ec4e-4d9a-bf54-04f4df0d8456",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetAirQualityCategoryForLocation(BaseModel):\n",
    "    \"\"\"Use external API to get current and accurate air quality category ('Fair', 'Poor', etc.) for a specified location.\"\"\"\n",
    "    latitude: float = Field(..., description=\"Latitude of the city.\")\n",
    "    longitude: float = Field(..., description=\"Longitude of the city.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc99af97-1af6-4f19-bd0b-3853f73701e8",
   "metadata": {},
   "source": [
    "We then decorate our function definition (copied from above) with the `tool` decorator, setting its `args_schema` to the Pydantic class we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d52f39-0911-4904-aaad-9c66b3ad67ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(args_schema=GetAirQualityCategoryForLocation)\n",
    "def get_air_quality_category_for_location(latitude, longitude) -> str:\n",
    "    base_url = \"https://air-quality-api.open-meteo.com/v1/air-quality\"\n",
    "    params = {\n",
    "        \"latitude\": latitude,\n",
    "        \"longitude\": longitude,\n",
    "        \"hourly\": \"european_aqi\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if \"hourly\" in data:\n",
    "            euro_aqi = data['hourly']['european_aqi'][0]\n",
    "            \n",
    "            # Determine AQI category\n",
    "            if euro_aqi <= 20:\n",
    "                return \"Good\"\n",
    "            elif euro_aqi <= 40:\n",
    "                return \"Fair\"\n",
    "            elif euro_aqi <= 60:\n",
    "                return \"Moderate\"\n",
    "            elif euro_aqi <= 80:\n",
    "                return \"Poor\"\n",
    "            elif euro_aqi <= 100:\n",
    "                return \"Very Poor\"\n",
    "            else:\n",
    "                return \"Extremely Poor\"\n",
    "        else:\n",
    "            return \"No air quality data found for the given coordinates.\"\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"An error occurred: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb13a7fb-bf0f-4483-9576-7037b64923b3",
   "metadata": {},
   "source": [
    "Since `get_air_quality_category_for_location` is now a `tool`, we ought to be able to `invoke` it with a dict mapping to its expected arguments (as we learned to do in the previous notebook) and get back a meaningful response, which we'll try here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfb782d-1172-4f82-89ec-056943a3e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_air_quality_category_for_location.invoke({'latitude': 19.0760, 'longitude': 72.8777})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d459aea-09df-4209-9f63-0c6be9b27451",
   "metadata": {},
   "source": [
    "### Create System Message for Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afb575c-a816-4304-885d-049cbbcafa06",
   "metadata": {},
   "source": [
    "If you were creating this agent from scratch, you would have eventually discovered on our test inputs that it had a tendency to discuss out loud that it wanted to use a the air quality tool, in a way that was not entirely helpful to the end user. Knowing what you do about iterative prompt engineering you would have then iterated on a system message to address this behavior.\n",
    "\n",
    "For the sake of this walkthrough we are going to spare you the process of iteratively developing an effective system message yourself and just provide you here with an effective system message we arrived at through an iterative process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83355cad-d219-41a6-8114-a2b81a5fecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\\\n",
    "You are a helpful assistant capable of tool calling when helpful, necessary, and appropriate.\n",
    "\n",
    "Think hard about whether or not you need to call a tool, \\\n",
    "based on your tools' descriptions and use them, but only when appropriate!\n",
    "\n",
    "Whether or not you need to call a tool, address the user's query in a helpful informative way.\n",
    "\n",
    "You should ALWAYS actually address the query and NEVER discuss your thought process about whether or not to use a tool.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e6fccd-6b42-43c5-9217-a7c48d01b181",
   "metadata": {},
   "source": [
    "### Create an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027ea09a-b207-418f-9d41-e13b1e3d5504",
   "metadata": {},
   "source": [
    "Now that you've created the tool you'd like your agent to utilize, a system message to guide its behavior, and an LLM instance (defined above as `llm`), you are ready to create an agent instance using `create_react_agent`.\n",
    "\n",
    "Feel free to check out the *Solution* below if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ac6ea1-2156-4a1d-ac42-1d8805d1bfd0",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd28178-596c-4df9-a276-c00767946a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab9f2968-ef5f-4ee5-b1b2-92a2ce566557",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0fbcaa-2c76-49cb-b602-9cb90eb48ac9",
   "metadata": {},
   "source": [
    "First we need to create a list containing the tool we would like our agent to have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5a533e-550b-4f41-93a4-470dcfd0789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_air_quality_category_for_location]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27c1825-3c68-4c2b-ba32-66a1bf35df80",
   "metadata": {},
   "source": [
    "Now we have everything we need to create an agent instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4af0e08-3be1-4aad-8592-15df72282982",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(llm, tools=tools, state_modifier=system_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fe39c5-431c-4ac3-bc1a-89eaa422222f",
   "metadata": {},
   "source": [
    "Let's sanity check that our agent works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e54924e-7401-4c4f-9660-88d1d9f1c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_state = agent.invoke({\"messages\": ['What is the current air quality in Mumbai?']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784e034d-75f5-4178-a454-db61de967c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in agent_state['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f14b61-f3cd-4a2c-9625-2603bec5cf5c",
   "metadata": {},
   "source": [
    "At least on our sample input, the agent appears to be working as expected, converting the location into the correct floating point latitude and longitude coordinates for use with the tool, invoking the tool, and using the results of the tool call in its final response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcee344-9585-41d1-9aff-04cf17f5d9fe",
   "metadata": {},
   "source": [
    "### Create Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d45f82-1a37-4d82-8ebf-da5f7b462701",
   "metadata": {},
   "source": [
    "Now let's create a chain that we can invoke with a simple string prompt and receive back a simple string response. You're welcome to re-use any of the code from earlier in the notebook.\n",
    "\n",
    "Feel free to check out the *Solution* below if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0c9f3-fc56-4f33-ae36-a4bf7557f08a",
   "metadata": {},
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cdf4ce-ccc0-4cf7-b35f-26627a972ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdc7e84b-5a2e-43c3-9a04-804bda602933",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3600a93b-5997-429e-9941-efd1e396c2af",
   "metadata": {},
   "source": [
    "We are going to use our new `agent` along with the `convert_to_agent_state` and `agent_state_parser` custom runnables we created earlier in the notebook to create our chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26950112-2108-40d5-9c3e-5e31b215c0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = convert_to_agent_state | agent | agent_state_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af120f2-9ae3-476f-8657-a23e26697a5d",
   "metadata": {},
   "source": [
    "Now that we have our runnable chain, let's test it out by sending it the test prompts, which we've copied here for easy reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debfa17c-8a80-4b7f-9493-d845ff83b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_quality_agent_test_prompts = [\n",
    "    \"What is the current air quality in Korobosea in Papua New Guinea?\",\n",
    "    \"What is the current air quality in Washington DC?\",\n",
    "    \"What is the current air quality in Mumbai?\",\n",
    "    \"Where is the city of Rome located?\" # Make sure agent behaves as expected when not needing to make a tool call.\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b2cd02-4bd3-44aa-9adf-6f5cb22dc729",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.batch(air_quality_agent_test_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3aa538-8562-463f-9b3b-4b8dbf7db2fc",
   "metadata": {},
   "source": [
    "Success!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
